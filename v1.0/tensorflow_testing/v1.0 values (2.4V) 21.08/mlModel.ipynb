{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"alltogether2.4v.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=[\"Material_Type\"])\n",
    "y = data[\"Material_Type\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALU' 'HDPE' 'LDPE' 'PP']\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# Normalize features\n",
    "X_train_normalized = (X_train - X_train.min()) / (X_train.max() - X_train.min())\n",
    "X_test_normalized = (X_test - X_train.min()) / (X_train.max() - X_train.min())\n",
    "\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 213ms/step - loss: 1.3956 - accuracy: 0.3193 - val_loss: 1.3467 - val_accuracy: 0.3333\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.3595 - accuracy: 0.3277 - val_loss: 1.3023 - val_accuracy: 0.6000\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.3198 - accuracy: 0.5126 - val_loss: 1.2644 - val_accuracy: 0.6000\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.2873 - accuracy: 0.5210 - val_loss: 1.2313 - val_accuracy: 0.6333\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.2604 - accuracy: 0.5546 - val_loss: 1.1990 - val_accuracy: 0.7333\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.2325 - accuracy: 0.5882 - val_loss: 1.1682 - val_accuracy: 0.7667\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.2069 - accuracy: 0.6134 - val_loss: 1.1391 - val_accuracy: 0.7000\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1789 - accuracy: 0.6134 - val_loss: 1.1089 - val_accuracy: 0.6333\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.1522 - accuracy: 0.6218 - val_loss: 1.0801 - val_accuracy: 0.6000\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.1249 - accuracy: 0.6218 - val_loss: 1.0529 - val_accuracy: 0.6000\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.0993 - accuracy: 0.6218 - val_loss: 1.0260 - val_accuracy: 0.6000\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.0739 - accuracy: 0.6218 - val_loss: 0.9997 - val_accuracy: 0.6333\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.0492 - accuracy: 0.6134 - val_loss: 0.9760 - val_accuracy: 0.6000\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.0262 - accuracy: 0.6050 - val_loss: 0.9509 - val_accuracy: 0.6333\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.0051 - accuracy: 0.5882 - val_loss: 0.9260 - val_accuracy: 0.7000\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.9838 - accuracy: 0.6050 - val_loss: 0.9052 - val_accuracy: 0.7000\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.9646 - accuracy: 0.6134 - val_loss: 0.8858 - val_accuracy: 0.6667\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.9466 - accuracy: 0.6050 - val_loss: 0.8668 - val_accuracy: 0.6333\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.9301 - accuracy: 0.6134 - val_loss: 0.8506 - val_accuracy: 0.6667\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.9149 - accuracy: 0.5882 - val_loss: 0.8372 - val_accuracy: 0.6667\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.9004 - accuracy: 0.6134 - val_loss: 0.8234 - val_accuracy: 0.6333\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.8883 - accuracy: 0.6134 - val_loss: 0.8113 - val_accuracy: 0.6333\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.8772 - accuracy: 0.6218 - val_loss: 0.7992 - val_accuracy: 0.6667\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.8674 - accuracy: 0.6303 - val_loss: 0.7893 - val_accuracy: 0.6333\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.8560 - accuracy: 0.6134 - val_loss: 0.7826 - val_accuracy: 0.6667\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.8473 - accuracy: 0.6050 - val_loss: 0.7731 - val_accuracy: 0.6667\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.8372 - accuracy: 0.6050 - val_loss: 0.7680 - val_accuracy: 0.7000\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8276 - accuracy: 0.6050 - val_loss: 0.7615 - val_accuracy: 0.7000\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8194 - accuracy: 0.6050 - val_loss: 0.7551 - val_accuracy: 0.6667\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.8118 - accuracy: 0.6050 - val_loss: 0.7466 - val_accuracy: 0.7000\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.8041 - accuracy: 0.6050 - val_loss: 0.7357 - val_accuracy: 0.7000\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.7953 - accuracy: 0.6134 - val_loss: 0.7276 - val_accuracy: 0.7000\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7876 - accuracy: 0.6387 - val_loss: 0.7230 - val_accuracy: 0.7333\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7808 - accuracy: 0.6387 - val_loss: 0.7177 - val_accuracy: 0.7000\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7750 - accuracy: 0.6134 - val_loss: 0.7115 - val_accuracy: 0.7000\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.7656 - accuracy: 0.6050 - val_loss: 0.7054 - val_accuracy: 0.7000\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7592 - accuracy: 0.6387 - val_loss: 0.6970 - val_accuracy: 0.7333\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7517 - accuracy: 0.6387 - val_loss: 0.6900 - val_accuracy: 0.7333\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7471 - accuracy: 0.6471 - val_loss: 0.6838 - val_accuracy: 0.7333\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7395 - accuracy: 0.6471 - val_loss: 0.6797 - val_accuracy: 0.7000\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7324 - accuracy: 0.6555 - val_loss: 0.6792 - val_accuracy: 0.6667\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.7306 - accuracy: 0.6471 - val_loss: 0.6793 - val_accuracy: 0.6667\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7218 - accuracy: 0.6387 - val_loss: 0.6642 - val_accuracy: 0.7000\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.7145 - accuracy: 0.6471 - val_loss: 0.6512 - val_accuracy: 0.7333\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.7104 - accuracy: 0.6471 - val_loss: 0.6454 - val_accuracy: 0.7333\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.7043 - accuracy: 0.6555 - val_loss: 0.6429 - val_accuracy: 0.7333\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6991 - accuracy: 0.6807 - val_loss: 0.6456 - val_accuracy: 0.7333\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6949 - accuracy: 0.6891 - val_loss: 0.6512 - val_accuracy: 0.7000\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6896 - accuracy: 0.6723 - val_loss: 0.6375 - val_accuracy: 0.7333\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6824 - accuracy: 0.6975 - val_loss: 0.6273 - val_accuracy: 0.7667\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6767 - accuracy: 0.6891 - val_loss: 0.6169 - val_accuracy: 0.7667\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6774 - accuracy: 0.6891 - val_loss: 0.6161 - val_accuracy: 0.7333\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6681 - accuracy: 0.7143 - val_loss: 0.6088 - val_accuracy: 0.7667\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6645 - accuracy: 0.6975 - val_loss: 0.6114 - val_accuracy: 0.7667\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6571 - accuracy: 0.7059 - val_loss: 0.6056 - val_accuracy: 0.7667\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6537 - accuracy: 0.7143 - val_loss: 0.6024 - val_accuracy: 0.7333\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6490 - accuracy: 0.7227 - val_loss: 0.5966 - val_accuracy: 0.7333\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6429 - accuracy: 0.7311 - val_loss: 0.5893 - val_accuracy: 0.7667\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6406 - accuracy: 0.7143 - val_loss: 0.5798 - val_accuracy: 0.7667\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6341 - accuracy: 0.7227 - val_loss: 0.5799 - val_accuracy: 0.7667\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6295 - accuracy: 0.7143 - val_loss: 0.5846 - val_accuracy: 0.7667\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6258 - accuracy: 0.7311 - val_loss: 0.5781 - val_accuracy: 0.7333\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6243 - accuracy: 0.7227 - val_loss: 0.5621 - val_accuracy: 0.8000\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6153 - accuracy: 0.7563 - val_loss: 0.5588 - val_accuracy: 0.7667\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6099 - accuracy: 0.7563 - val_loss: 0.5613 - val_accuracy: 0.7333\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6050 - accuracy: 0.7395 - val_loss: 0.5585 - val_accuracy: 0.7333\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6004 - accuracy: 0.7395 - val_loss: 0.5515 - val_accuracy: 0.7333\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6046 - accuracy: 0.7479 - val_loss: 0.5427 - val_accuracy: 0.8000\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5920 - accuracy: 0.7395 - val_loss: 0.5427 - val_accuracy: 0.7333\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5862 - accuracy: 0.7479 - val_loss: 0.5377 - val_accuracy: 0.7667\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5883 - accuracy: 0.7647 - val_loss: 0.5291 - val_accuracy: 0.8333\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5776 - accuracy: 0.7647 - val_loss: 0.5334 - val_accuracy: 0.8000\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5744 - accuracy: 0.7563 - val_loss: 0.5311 - val_accuracy: 0.7333\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5707 - accuracy: 0.7647 - val_loss: 0.5207 - val_accuracy: 0.8000\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5665 - accuracy: 0.7815 - val_loss: 0.5127 - val_accuracy: 0.9000\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5589 - accuracy: 0.7899 - val_loss: 0.5118 - val_accuracy: 0.9000\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5573 - accuracy: 0.7647 - val_loss: 0.5126 - val_accuracy: 0.7667\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5497 - accuracy: 0.7731 - val_loss: 0.5006 - val_accuracy: 0.9000\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5448 - accuracy: 0.7899 - val_loss: 0.4952 - val_accuracy: 0.8667\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5408 - accuracy: 0.7983 - val_loss: 0.4917 - val_accuracy: 0.8667\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5359 - accuracy: 0.7983 - val_loss: 0.4898 - val_accuracy: 0.8667\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5337 - accuracy: 0.7983 - val_loss: 0.4897 - val_accuracy: 0.8667\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5308 - accuracy: 0.7815 - val_loss: 0.4813 - val_accuracy: 0.9000\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.5259 - accuracy: 0.7983 - val_loss: 0.4721 - val_accuracy: 0.9000\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5191 - accuracy: 0.8151 - val_loss: 0.4749 - val_accuracy: 0.8333\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5164 - accuracy: 0.8067 - val_loss: 0.4842 - val_accuracy: 0.7333\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5112 - accuracy: 0.7899 - val_loss: 0.4702 - val_accuracy: 0.8333\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5031 - accuracy: 0.8235 - val_loss: 0.4570 - val_accuracy: 0.9000\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5034 - accuracy: 0.8235 - val_loss: 0.4517 - val_accuracy: 0.8667\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4950 - accuracy: 0.8235 - val_loss: 0.4571 - val_accuracy: 0.8667\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4897 - accuracy: 0.8235 - val_loss: 0.4625 - val_accuracy: 0.8333\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4901 - accuracy: 0.8151 - val_loss: 0.4507 - val_accuracy: 0.8667\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4806 - accuracy: 0.8235 - val_loss: 0.4412 - val_accuracy: 0.9000\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4775 - accuracy: 0.8319 - val_loss: 0.4344 - val_accuracy: 0.8667\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4722 - accuracy: 0.8319 - val_loss: 0.4359 - val_accuracy: 0.8667\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4709 - accuracy: 0.8319 - val_loss: 0.4362 - val_accuracy: 0.8667\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4646 - accuracy: 0.8235 - val_loss: 0.4273 - val_accuracy: 0.8667\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4589 - accuracy: 0.8571 - val_loss: 0.4184 - val_accuracy: 0.8667\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4584 - accuracy: 0.8655 - val_loss: 0.4186 - val_accuracy: 0.8667\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4506 - accuracy: 0.8571 - val_loss: 0.4201 - val_accuracy: 0.8667\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4467 - accuracy: 0.8571 - val_loss: 0.4178 - val_accuracy: 0.8667\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4418 - accuracy: 0.8571 - val_loss: 0.4127 - val_accuracy: 0.8667\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4358 - accuracy: 0.8487 - val_loss: 0.4020 - val_accuracy: 0.8667\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4356 - accuracy: 0.8655 - val_loss: 0.3963 - val_accuracy: 0.8667\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4285 - accuracy: 0.8655 - val_loss: 0.4035 - val_accuracy: 0.8667\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4239 - accuracy: 0.8571 - val_loss: 0.4002 - val_accuracy: 0.9000\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4193 - accuracy: 0.8655 - val_loss: 0.3914 - val_accuracy: 0.8667\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4144 - accuracy: 0.8739 - val_loss: 0.3885 - val_accuracy: 0.8667\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4115 - accuracy: 0.8739 - val_loss: 0.3864 - val_accuracy: 0.9000\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4059 - accuracy: 0.8655 - val_loss: 0.3892 - val_accuracy: 0.9000\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4043 - accuracy: 0.8655 - val_loss: 0.3763 - val_accuracy: 0.8667\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4009 - accuracy: 0.8908 - val_loss: 0.3713 - val_accuracy: 0.8667\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3940 - accuracy: 0.8908 - val_loss: 0.3792 - val_accuracy: 0.8667\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3934 - accuracy: 0.8571 - val_loss: 0.3766 - val_accuracy: 0.8667\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3870 - accuracy: 0.8824 - val_loss: 0.3620 - val_accuracy: 0.8667\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3932 - accuracy: 0.8824 - val_loss: 0.3583 - val_accuracy: 0.8667\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3809 - accuracy: 0.8824 - val_loss: 0.3716 - val_accuracy: 0.9000\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3773 - accuracy: 0.8655 - val_loss: 0.3688 - val_accuracy: 0.8333\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3777 - accuracy: 0.8739 - val_loss: 0.3598 - val_accuracy: 0.8333\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3722 - accuracy: 0.8655 - val_loss: 0.3486 - val_accuracy: 0.9000\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3630 - accuracy: 0.8824 - val_loss: 0.3535 - val_accuracy: 0.8667\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3636 - accuracy: 0.8824 - val_loss: 0.3546 - val_accuracy: 0.8667\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3648 - accuracy: 0.8739 - val_loss: 0.3437 - val_accuracy: 0.8667\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3541 - accuracy: 0.8824 - val_loss: 0.3425 - val_accuracy: 0.9000\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3507 - accuracy: 0.8824 - val_loss: 0.3412 - val_accuracy: 0.9000\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3489 - accuracy: 0.8908 - val_loss: 0.3395 - val_accuracy: 0.9000\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.3484 - accuracy: 0.8908 - val_loss: 0.3345 - val_accuracy: 0.8667\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3409 - accuracy: 0.8908 - val_loss: 0.3394 - val_accuracy: 0.9000\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3395 - accuracy: 0.8824 - val_loss: 0.3388 - val_accuracy: 0.9000\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3375 - accuracy: 0.8824 - val_loss: 0.3284 - val_accuracy: 0.8667\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3317 - accuracy: 0.8908 - val_loss: 0.3280 - val_accuracy: 0.9000\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3298 - accuracy: 0.8824 - val_loss: 0.3290 - val_accuracy: 0.9000\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3259 - accuracy: 0.8824 - val_loss: 0.3320 - val_accuracy: 0.9000\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3251 - accuracy: 0.8824 - val_loss: 0.3256 - val_accuracy: 0.9000\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3200 - accuracy: 0.8908 - val_loss: 0.3194 - val_accuracy: 0.9000\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3196 - accuracy: 0.8824 - val_loss: 0.3200 - val_accuracy: 0.8667\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3161 - accuracy: 0.8908 - val_loss: 0.3187 - val_accuracy: 0.9000\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3138 - accuracy: 0.8824 - val_loss: 0.3189 - val_accuracy: 0.8667\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3109 - accuracy: 0.8908 - val_loss: 0.3190 - val_accuracy: 0.9000\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3076 - accuracy: 0.8908 - val_loss: 0.3126 - val_accuracy: 0.8667\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3057 - accuracy: 0.8992 - val_loss: 0.3142 - val_accuracy: 0.9000\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3054 - accuracy: 0.8908 - val_loss: 0.3134 - val_accuracy: 0.9000\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3018 - accuracy: 0.8824 - val_loss: 0.3068 - val_accuracy: 0.8667\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2983 - accuracy: 0.8992 - val_loss: 0.3058 - val_accuracy: 0.8667\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2934 - accuracy: 0.8992 - val_loss: 0.3101 - val_accuracy: 0.9000\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2916 - accuracy: 0.8992 - val_loss: 0.3122 - val_accuracy: 0.8667\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2906 - accuracy: 0.8992 - val_loss: 0.3070 - val_accuracy: 0.9000\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2870 - accuracy: 0.8992 - val_loss: 0.3013 - val_accuracy: 0.9000\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2834 - accuracy: 0.8992 - val_loss: 0.2988 - val_accuracy: 0.8667\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2843 - accuracy: 0.8992 - val_loss: 0.3008 - val_accuracy: 0.8667\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2810 - accuracy: 0.8992 - val_loss: 0.2995 - val_accuracy: 0.8667\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2801 - accuracy: 0.9076 - val_loss: 0.3025 - val_accuracy: 0.9000\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2753 - accuracy: 0.8992 - val_loss: 0.2987 - val_accuracy: 0.9000\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2757 - accuracy: 0.8992 - val_loss: 0.2961 - val_accuracy: 0.9000\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2724 - accuracy: 0.9160 - val_loss: 0.2945 - val_accuracy: 0.8667\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2713 - accuracy: 0.8992 - val_loss: 0.2952 - val_accuracy: 0.9000\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2696 - accuracy: 0.9160 - val_loss: 0.2986 - val_accuracy: 0.9000\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2670 - accuracy: 0.9076 - val_loss: 0.2953 - val_accuracy: 0.9000\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2637 - accuracy: 0.9244 - val_loss: 0.2901 - val_accuracy: 0.8667\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2606 - accuracy: 0.9160 - val_loss: 0.2933 - val_accuracy: 0.8667\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2602 - accuracy: 0.9160 - val_loss: 0.2927 - val_accuracy: 0.8667\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.2565 - accuracy: 0.9160 - val_loss: 0.2886 - val_accuracy: 0.9000\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2554 - accuracy: 0.9244 - val_loss: 0.2893 - val_accuracy: 0.9000\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2532 - accuracy: 0.9244 - val_loss: 0.2946 - val_accuracy: 0.8667\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2528 - accuracy: 0.9244 - val_loss: 0.2893 - val_accuracy: 0.9000\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2482 - accuracy: 0.9244 - val_loss: 0.2830 - val_accuracy: 0.8667\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2482 - accuracy: 0.9244 - val_loss: 0.2821 - val_accuracy: 0.8667\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2445 - accuracy: 0.9244 - val_loss: 0.2887 - val_accuracy: 0.8667\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2456 - accuracy: 0.9244 - val_loss: 0.2911 - val_accuracy: 0.9000\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2416 - accuracy: 0.9244 - val_loss: 0.2839 - val_accuracy: 0.8667\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2396 - accuracy: 0.9244 - val_loss: 0.2811 - val_accuracy: 0.8667\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2388 - accuracy: 0.9160 - val_loss: 0.2828 - val_accuracy: 0.8667\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.2347 - accuracy: 0.9244 - val_loss: 0.2888 - val_accuracy: 0.9000\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.2373 - accuracy: 0.9244 - val_loss: 0.2899 - val_accuracy: 0.8667\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2334 - accuracy: 0.9328 - val_loss: 0.2773 - val_accuracy: 0.9000\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.2328 - accuracy: 0.9244 - val_loss: 0.2765 - val_accuracy: 0.8667\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2328 - accuracy: 0.9160 - val_loss: 0.2818 - val_accuracy: 0.9333\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2266 - accuracy: 0.9244 - val_loss: 0.2809 - val_accuracy: 0.8667\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2289 - accuracy: 0.9244 - val_loss: 0.2789 - val_accuracy: 0.8667\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.2256 - accuracy: 0.9244 - val_loss: 0.2858 - val_accuracy: 0.9000\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2230 - accuracy: 0.9244 - val_loss: 0.2819 - val_accuracy: 0.8667\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2206 - accuracy: 0.9244 - val_loss: 0.2756 - val_accuracy: 0.8667\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2201 - accuracy: 0.9244 - val_loss: 0.2763 - val_accuracy: 0.9000\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2182 - accuracy: 0.9244 - val_loss: 0.2835 - val_accuracy: 0.9000\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2183 - accuracy: 0.9244 - val_loss: 0.2788 - val_accuracy: 0.9000\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2133 - accuracy: 0.9244 - val_loss: 0.2784 - val_accuracy: 0.9000\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2184 - accuracy: 0.9244 - val_loss: 0.2752 - val_accuracy: 0.9000\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2131 - accuracy: 0.9328 - val_loss: 0.2826 - val_accuracy: 0.9000\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2114 - accuracy: 0.9412 - val_loss: 0.2777 - val_accuracy: 0.9000\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2082 - accuracy: 0.9244 - val_loss: 0.2733 - val_accuracy: 0.8667\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2081 - accuracy: 0.9244 - val_loss: 0.2757 - val_accuracy: 0.8667\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2042 - accuracy: 0.9244 - val_loss: 0.2782 - val_accuracy: 0.9000\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2038 - accuracy: 0.9328 - val_loss: 0.2775 - val_accuracy: 0.9000\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2042 - accuracy: 0.9328 - val_loss: 0.2737 - val_accuracy: 0.9000\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1996 - accuracy: 0.9244 - val_loss: 0.2695 - val_accuracy: 0.8667\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2015 - accuracy: 0.9328 - val_loss: 0.2704 - val_accuracy: 0.9000\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1983 - accuracy: 0.9244 - val_loss: 0.2803 - val_accuracy: 0.9000\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1976 - accuracy: 0.9496 - val_loss: 0.2793 - val_accuracy: 0.9000\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1976 - accuracy: 0.9328 - val_loss: 0.2707 - val_accuracy: 0.9333\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1949 - accuracy: 0.9328 - val_loss: 0.2718 - val_accuracy: 0.9333\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1997 - accuracy: 0.9211\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train_normalized, y_train, epochs=200, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "# history = model.fit(X_train_normalized, y_train, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_normalized, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n",
      "Test Accuracy: 0.9210526347160339\n",
      "Sample 1: Correct Predicted ALU with 98.69% likelihood\n",
      "Sample 2: Correct Predicted LDPE with 77.81% likelihood\n",
      "Sample 3: Correct Predicted LDPE with 64.19% likelihood\n",
      "Sample 4: Correct Predicted PP with 89.57% likelihood\n",
      "Sample 5: Correct Predicted ALU with 99.99% likelihood\n",
      "Sample 6: Correct Predicted HDPE with 99.48% likelihood\n",
      "Sample 7: Correct Predicted LDPE with 59.81% likelihood\n",
      "Sample 8: Correct Predicted ALU with 99.99% likelihood\n",
      "Sample 9: Correct Predicted PP with 57.04% likelihood\n",
      "Sample 10: Correct Predicted ALU with 99.97% likelihood\n",
      "Sample 11: Correct Predicted ALU with 99.99% likelihood\n",
      "Sample 12: Correct Predicted HDPE with 85.33% likelihood\n",
      "Sample 13: Correct Predicted ALU with 99.96% likelihood\n",
      "Sample 14: Correct Predicted HDPE with 87.02% likelihood\n",
      "Sample 15: Correct Predicted LDPE with 95.59% likelihood\n",
      "Total size of the model: 74.52 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kylej\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test_normalized)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_plastic_types = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Convert predictions to percentage likelihood\n",
    "percentage_likelihood = predictions.max(axis=1) * 100\n",
    "\n",
    "# Display test accuracy and example predictions\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "for i in range(15):\n",
    "    if y_test[i] == predicted_labels[i]:\n",
    "        accuracy = \"Correct\"\n",
    "    else:\n",
    "        accuracy = \"False\"\n",
    "    print(f\"Sample {i+1}: {accuracy} Predicted {predicted_plastic_types[i]} with {percentage_likelihood[i]:.2f}% likelihood\")\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save(\"plastic_classifier_model.h5\")\n",
    "\n",
    "total_params = model.count_params()\n",
    "total_size_bytes = total_params * 4  # Each parameter is usually a 32-bit float\n",
    "total_size_mb = total_size_bytes / (1024)  # Convert bytes to megabytes\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "[[3 2 1 0]\n",
      " [0 1 3 2]\n",
      " [0 1 3 2]\n",
      " [0 1 2 3]\n",
      " [3 1 2 0]\n",
      " [0 2 3 1]\n",
      " [0 1 3 2]\n",
      " [3 1 2 0]\n",
      " [1 0 2 3]\n",
      " [3 1 2 0]\n",
      " [3 1 2 0]\n",
      " [0 2 3 1]\n",
      " [3 1 2 0]\n",
      " [0 3 2 1]\n",
      " [1 0 3 2]\n",
      " [3 0 2 1]\n",
      " [3 1 2 0]\n",
      " [3 1 2 0]\n",
      " [1 0 2 3]\n",
      " [0 2 3 1]\n",
      " [0 1 2 3]\n",
      " [0 1 3 2]\n",
      " [0 3 2 1]\n",
      " [3 2 1 0]\n",
      " [0 2 3 1]\n",
      " [3 2 1 0]\n",
      " [0 2 3 1]\n",
      " [1 0 2 3]\n",
      " [1 0 2 3]\n",
      " [3 1 2 0]\n",
      " [0 2 3 1]\n",
      " [3 1 2 0]\n",
      " [0 1 2 3]\n",
      " [0 1 3 2]\n",
      " [0 1 3 2]\n",
      " [0 3 2 1]\n",
      " [0 1 2 3]\n",
      " [3 2 1 0]]\n",
      "Test Accuracy: 0.9210526347160339\n",
      "Sample 1:\n",
      "  Prediction 1: ALU with 0.00% likelihood\n",
      "  Prediction 2: HDPE with 0.16% likelihood\n",
      "  Prediction 3: LDPE with 1.15% likelihood\n",
      "  Prediction 4: PP with 98.69% likelihood\n",
      "Sample 2:\n",
      "  Prediction 1: LDPE with 0.10% likelihood\n",
      "  Prediction 2: PP with 0.13% likelihood\n",
      "  Prediction 3: HDPE with 21.96% likelihood\n",
      "  Prediction 4: ALU with 77.81% likelihood\n",
      "Sample 3:\n",
      "  Prediction 1: LDPE with 0.02% likelihood\n",
      "  Prediction 2: PP with 0.77% likelihood\n",
      "  Prediction 3: HDPE with 35.02% likelihood\n",
      "  Prediction 4: ALU with 64.19% likelihood\n",
      "Sample 4:\n",
      "  Prediction 1: PP with 0.02% likelihood\n",
      "  Prediction 2: LDPE with 0.02% likelihood\n",
      "  Prediction 3: HDPE with 10.39% likelihood\n",
      "  Prediction 4: ALU with 89.57% likelihood\n",
      "Sample 5:\n",
      "  Prediction 1: ALU with 0.00% likelihood\n",
      "  Prediction 2: LDPE with 0.00% likelihood\n",
      "  Prediction 3: HDPE with 0.01% likelihood\n",
      "  Prediction 4: PP with 99.99% likelihood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kylej\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test_normalized)\n",
    "\n",
    "# Get the top 4 predictions for each sample\n",
    "top_predictions = np.argsort(predictions, axis=1)[:, -4:]\n",
    "top_labels = top_predictions[:, ::-1]\n",
    "\n",
    "print(top_predictions)\n",
    "\n",
    "# Reshape the top_labels array to be 1D\n",
    "top_labels_1d = top_labels.reshape(-1)\n",
    "\n",
    "# Inverse transform the labels\n",
    "top_plastic_types = label_encoder.inverse_transform(top_labels_1d)\n",
    "\n",
    "# Reshape the top_plastic_types array back to 2D\n",
    "top_plastic_types = top_plastic_types.reshape(top_labels.shape)\n",
    "\n",
    "# Get the percentage likelihoods for the top 4 predictions\n",
    "percentage_likelihoods = np.take_along_axis(predictions, top_predictions, axis=1) * 100\n",
    "\n",
    "# Display test accuracy and example predictions\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "for i in range(5):\n",
    "    num_predictions = min(4, len(np.unique(top_labels[i])))\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    for j in range(num_predictions):\n",
    "        print(f\"  Prediction {j+1}: {top_plastic_types[i][j]} with {percentage_likelihoods[i][j]:.2f}% likelihood\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"plastic_classifier_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "[[6.1377513e-01 2.1493343e-04 5.9797116e-02 3.2621279e-01]]\n",
      "0\n",
      "ALU\n",
      "Manual Input Prediction:\n",
      "  Prediction: ALU with 61.38% likelihood\n"
     ]
    }
   ],
   "source": [
    "# Get manual input from the user\n",
    "manual_input = np.array([[524161,1534725,2819437,3403538,4467703,2420628,2047009,1183458,860611,1677583,2394933,3207380,2819927,1889676,1374559]])  # Replace with your input values\n",
    "\n",
    "# Normalize the manual input if needed\n",
    "normalized_manual_input = ((manual_input - 0) / (8388608 - 0))\n",
    "\n",
    "# Make predictions on manual input\n",
    "predictions = model.predict(normalized_manual_input)\n",
    "print(predictions)\n",
    "\n",
    "# Get the top prediction\n",
    "top_prediction = np.argmax(predictions)\n",
    "print(top_prediction)\n",
    "\n",
    "top_plastic_type = label_encoder.inverse_transform([top_prediction])[0]\n",
    "print(top_plastic_type)\n",
    "likelihood = predictions[0][top_prediction] * 100\n",
    "\n",
    "# Display the prediction for manual input\n",
    "print(f\"Manual Input Prediction:\")\n",
    "print(f\"  Prediction: {top_plastic_type} with {likelihood:.2f}% likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "Predicted label: PP\n",
      "True label: ALU\n",
      "Accuracy: False\n",
      "PP: 97.26% likelihood\n",
      "LDPE: 2.16% likelihood\n",
      "HDPE: 0.51% likelihood\n",
      "ALU: 0.07% likelihood\n"
     ]
    }
   ],
   "source": [
    "# Make predictions from manual\n",
    "manual_input = np.array([[594978, 1846783, 3119048, 3857789, 5733774, 3304025, 2417774, 1433893, 947344, 2020815, 2890104, 4187639, 3930384, 2542075, 1737344]])\n",
    "\n",
    "# Calculate the column-wise min and max values from your original dataset\n",
    "min_values = [74296, 1151721, 2379300, 2748695, 3681840, 1997402, 1742243, 942818, 392145, 795677, 1406647, 2482140, 2630495, 1748806, 1249297]\n",
    "max_values = [1773075, 3755206, 7787954, 8401566, 8405071, 8401566, 7232265, 8185313, 2080604, 4455589, 8402979, 8405071, 8405071, 8403086, 6084492]\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "min_values = np.array(min_values)\n",
    "max_values = np.array(max_values)\n",
    "\n",
    "# Normalize the manual input using the calculated min and max values\n",
    "normalized_manual_input = (manual_input - min_values) / (max_values - min_values)\n",
    "\n",
    "predictions = model.predict(normalized_manual_input)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_plastic_types = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "    \n",
    "## Calculate the percentage likelihoods\n",
    "percentage_likelihoods = predictions * 100\n",
    "\n",
    "# Get the indices of the top 4 predicted classes based on percentage likelihoods\n",
    "top_indices = np.argsort(percentage_likelihoods[0])[::-1][:4]\n",
    "\n",
    "# Get the labels for the top predicted classes\n",
    "top_labels = label_encoder.inverse_transform(top_indices)\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = top_labels[0]\n",
    "\n",
    "# Get the true label from the original data (replace with your true label)\n",
    "true_label = 'ALU'  # Replace with the true label\n",
    "\n",
    "# Calculate the accuracy\n",
    "if predicted_label == true_label:\n",
    "    accuracy = \"Correct\"\n",
    "else:\n",
    "    accuracy = \"False\"\n",
    "\n",
    "# Print the prediction, label, and accuracy\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Print the percentages for each type\n",
    "for label, likelihood in zip(top_labels, percentage_likelihoods[0][top_indices]):\n",
    "    print(f\"{label}: {likelihood:.2f}% likelihood\")\n",
    "\n",
    "# ... (rest of the code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
