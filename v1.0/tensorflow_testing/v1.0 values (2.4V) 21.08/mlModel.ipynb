{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"alltogether2.4v.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=[\"Material_Type\"])\n",
    "y = data[\"Material_Type\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALU' 'HDPE' 'LDPE' 'PP']\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# Normalize features\n",
    "X_train_normalized = (X_train - X_train.min()) / (X_train.max() - X_train.min())\n",
    "X_test_normalized = (X_test - X_train.min()) / (X_train.max() - X_train.min())\n",
    "\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 137ms/step - loss: 1.4527 - accuracy: 0.2750 - val_loss: 1.4261 - val_accuracy: 0.1935\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.3954 - accuracy: 0.2667 - val_loss: 1.3563 - val_accuracy: 0.3548\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.3510 - accuracy: 0.3250 - val_loss: 1.3046 - val_accuracy: 0.3226\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.3217 - accuracy: 0.2333 - val_loss: 1.2654 - val_accuracy: 0.3226\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.2916 - accuracy: 0.2250 - val_loss: 1.2333 - val_accuracy: 0.3226\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.2699 - accuracy: 0.2500 - val_loss: 1.2058 - val_accuracy: 0.3548\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.2471 - accuracy: 0.3250 - val_loss: 1.1809 - val_accuracy: 0.3548\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.2238 - accuracy: 0.4583 - val_loss: 1.1588 - val_accuracy: 0.5806\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.2001 - accuracy: 0.5750 - val_loss: 1.1353 - val_accuracy: 0.6452\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.1756 - accuracy: 0.6083 - val_loss: 1.1118 - val_accuracy: 0.5806\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.1501 - accuracy: 0.6083 - val_loss: 1.0867 - val_accuracy: 0.5806\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.1248 - accuracy: 0.6417 - val_loss: 1.0631 - val_accuracy: 0.5806\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.0979 - accuracy: 0.6417 - val_loss: 1.0357 - val_accuracy: 0.5806\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0718 - accuracy: 0.6417 - val_loss: 1.0092 - val_accuracy: 0.5806\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0468 - accuracy: 0.6417 - val_loss: 0.9815 - val_accuracy: 0.5806\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.0221 - accuracy: 0.6333 - val_loss: 0.9563 - val_accuracy: 0.5806\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9992 - accuracy: 0.6333 - val_loss: 0.9314 - val_accuracy: 0.5806\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9769 - accuracy: 0.6417 - val_loss: 0.9115 - val_accuracy: 0.5806\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9563 - accuracy: 0.6500 - val_loss: 0.8927 - val_accuracy: 0.5806\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9369 - accuracy: 0.6417 - val_loss: 0.8734 - val_accuracy: 0.5806\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.9202 - accuracy: 0.6583 - val_loss: 0.8555 - val_accuracy: 0.6452\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9052 - accuracy: 0.6417 - val_loss: 0.8391 - val_accuracy: 0.6452\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.8898 - accuracy: 0.6417 - val_loss: 0.8272 - val_accuracy: 0.6452\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8770 - accuracy: 0.6417 - val_loss: 0.8189 - val_accuracy: 0.6452\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.8642 - accuracy: 0.6500 - val_loss: 0.8090 - val_accuracy: 0.6452\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8532 - accuracy: 0.6500 - val_loss: 0.7964 - val_accuracy: 0.6452\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.8432 - accuracy: 0.6417 - val_loss: 0.7891 - val_accuracy: 0.6452\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8340 - accuracy: 0.6500 - val_loss: 0.7773 - val_accuracy: 0.6452\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8256 - accuracy: 0.6500 - val_loss: 0.7688 - val_accuracy: 0.6774\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8167 - accuracy: 0.6417 - val_loss: 0.7658 - val_accuracy: 0.6774\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.8076 - accuracy: 0.6667 - val_loss: 0.7612 - val_accuracy: 0.6452\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8001 - accuracy: 0.6750 - val_loss: 0.7556 - val_accuracy: 0.6452\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7937 - accuracy: 0.6583 - val_loss: 0.7488 - val_accuracy: 0.6452\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.7868 - accuracy: 0.6500 - val_loss: 0.7395 - val_accuracy: 0.6774\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7812 - accuracy: 0.6583 - val_loss: 0.7358 - val_accuracy: 0.6452\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7740 - accuracy: 0.6500 - val_loss: 0.7318 - val_accuracy: 0.6774\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7671 - accuracy: 0.6583 - val_loss: 0.7302 - val_accuracy: 0.6774\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7618 - accuracy: 0.6833 - val_loss: 0.7286 - val_accuracy: 0.6129\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7555 - accuracy: 0.6750 - val_loss: 0.7243 - val_accuracy: 0.6452\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7494 - accuracy: 0.6667 - val_loss: 0.7188 - val_accuracy: 0.6774\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7441 - accuracy: 0.6750 - val_loss: 0.7105 - val_accuracy: 0.6774\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7378 - accuracy: 0.6750 - val_loss: 0.7082 - val_accuracy: 0.6774\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7311 - accuracy: 0.6833 - val_loss: 0.7048 - val_accuracy: 0.6774\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7266 - accuracy: 0.7000 - val_loss: 0.6947 - val_accuracy: 0.6774\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.7187 - accuracy: 0.6750 - val_loss: 0.6930 - val_accuracy: 0.6774\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7125 - accuracy: 0.6833 - val_loss: 0.6886 - val_accuracy: 0.6774\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.7067 - accuracy: 0.7083 - val_loss: 0.6915 - val_accuracy: 0.7097\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7026 - accuracy: 0.7000 - val_loss: 0.6878 - val_accuracy: 0.6452\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6952 - accuracy: 0.7083 - val_loss: 0.6747 - val_accuracy: 0.7419\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6916 - accuracy: 0.6833 - val_loss: 0.6602 - val_accuracy: 0.7742\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6863 - accuracy: 0.6833 - val_loss: 0.6599 - val_accuracy: 0.7419\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6838 - accuracy: 0.7000 - val_loss: 0.6681 - val_accuracy: 0.7097\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6767 - accuracy: 0.7167 - val_loss: 0.6687 - val_accuracy: 0.7097\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6713 - accuracy: 0.7000 - val_loss: 0.6545 - val_accuracy: 0.7419\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6643 - accuracy: 0.6917 - val_loss: 0.6491 - val_accuracy: 0.7419\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6601 - accuracy: 0.7000 - val_loss: 0.6489 - val_accuracy: 0.7097\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6572 - accuracy: 0.7167 - val_loss: 0.6474 - val_accuracy: 0.7097\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6503 - accuracy: 0.7083 - val_loss: 0.6411 - val_accuracy: 0.7097\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6454 - accuracy: 0.7083 - val_loss: 0.6357 - val_accuracy: 0.7097\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6407 - accuracy: 0.7083 - val_loss: 0.6341 - val_accuracy: 0.7097\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6370 - accuracy: 0.7083 - val_loss: 0.6448 - val_accuracy: 0.7097\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6331 - accuracy: 0.7083 - val_loss: 0.6372 - val_accuracy: 0.7097\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6251 - accuracy: 0.7167 - val_loss: 0.6238 - val_accuracy: 0.7097\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6227 - accuracy: 0.7083 - val_loss: 0.6167 - val_accuracy: 0.7419\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6156 - accuracy: 0.7417 - val_loss: 0.6077 - val_accuracy: 0.7419\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6122 - accuracy: 0.7417 - val_loss: 0.6077 - val_accuracy: 0.7419\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6101 - accuracy: 0.7250 - val_loss: 0.6204 - val_accuracy: 0.7419\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6033 - accuracy: 0.7167 - val_loss: 0.6117 - val_accuracy: 0.7097\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6027 - accuracy: 0.7167 - val_loss: 0.6047 - val_accuracy: 0.7419\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5945 - accuracy: 0.7167 - val_loss: 0.6092 - val_accuracy: 0.7097\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5886 - accuracy: 0.7250 - val_loss: 0.6057 - val_accuracy: 0.7097\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5833 - accuracy: 0.7167 - val_loss: 0.5946 - val_accuracy: 0.7419\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5809 - accuracy: 0.7417 - val_loss: 0.5837 - val_accuracy: 0.7742\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5752 - accuracy: 0.7417 - val_loss: 0.5876 - val_accuracy: 0.7742\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5702 - accuracy: 0.7333 - val_loss: 0.5881 - val_accuracy: 0.7419\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5656 - accuracy: 0.7500 - val_loss: 0.5883 - val_accuracy: 0.7419\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5646 - accuracy: 0.7333 - val_loss: 0.5938 - val_accuracy: 0.7097\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5572 - accuracy: 0.7500 - val_loss: 0.5779 - val_accuracy: 0.7742\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5527 - accuracy: 0.7667 - val_loss: 0.5640 - val_accuracy: 0.7742\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5550 - accuracy: 0.7750 - val_loss: 0.5611 - val_accuracy: 0.7742\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5410 - accuracy: 0.7667 - val_loss: 0.5830 - val_accuracy: 0.7419\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5445 - accuracy: 0.7667 - val_loss: 0.5950 - val_accuracy: 0.6774\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5415 - accuracy: 0.7583 - val_loss: 0.5700 - val_accuracy: 0.7419\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5283 - accuracy: 0.7917 - val_loss: 0.5477 - val_accuracy: 0.8065\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5299 - accuracy: 0.7750 - val_loss: 0.5457 - val_accuracy: 0.8065\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5229 - accuracy: 0.7917 - val_loss: 0.5607 - val_accuracy: 0.7419\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5177 - accuracy: 0.7917 - val_loss: 0.5776 - val_accuracy: 0.7419\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5167 - accuracy: 0.7833 - val_loss: 0.5604 - val_accuracy: 0.7742\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5083 - accuracy: 0.8167 - val_loss: 0.5376 - val_accuracy: 0.7419\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5050 - accuracy: 0.8000 - val_loss: 0.5346 - val_accuracy: 0.7742\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5031 - accuracy: 0.8000 - val_loss: 0.5379 - val_accuracy: 0.8065\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4993 - accuracy: 0.8250 - val_loss: 0.5642 - val_accuracy: 0.7742\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4941 - accuracy: 0.8167 - val_loss: 0.5472 - val_accuracy: 0.7742\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4887 - accuracy: 0.8000 - val_loss: 0.5266 - val_accuracy: 0.7419\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4827 - accuracy: 0.8167 - val_loss: 0.5296 - val_accuracy: 0.7419\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4798 - accuracy: 0.8250 - val_loss: 0.5329 - val_accuracy: 0.8065\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4720 - accuracy: 0.8417 - val_loss: 0.5441 - val_accuracy: 0.7742\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4716 - accuracy: 0.8167 - val_loss: 0.5393 - val_accuracy: 0.7419\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4684 - accuracy: 0.8083 - val_loss: 0.5189 - val_accuracy: 0.8065\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4645 - accuracy: 0.8167 - val_loss: 0.5156 - val_accuracy: 0.8065\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4584 - accuracy: 0.8333 - val_loss: 0.5362 - val_accuracy: 0.8065\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4561 - accuracy: 0.8000 - val_loss: 0.5279 - val_accuracy: 0.8065\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4507 - accuracy: 0.8333 - val_loss: 0.5151 - val_accuracy: 0.7742\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4461 - accuracy: 0.8417 - val_loss: 0.5128 - val_accuracy: 0.8065\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4428 - accuracy: 0.8583 - val_loss: 0.5203 - val_accuracy: 0.8065\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4360 - accuracy: 0.8333 - val_loss: 0.5133 - val_accuracy: 0.8065\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4366 - accuracy: 0.8417 - val_loss: 0.5063 - val_accuracy: 0.8387\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4310 - accuracy: 0.8500 - val_loss: 0.5059 - val_accuracy: 0.8387\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4272 - accuracy: 0.8417 - val_loss: 0.5119 - val_accuracy: 0.8065\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4234 - accuracy: 0.8500 - val_loss: 0.5132 - val_accuracy: 0.8065\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4185 - accuracy: 0.8583 - val_loss: 0.5120 - val_accuracy: 0.8065\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4149 - accuracy: 0.8500 - val_loss: 0.4999 - val_accuracy: 0.8387\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4128 - accuracy: 0.8667 - val_loss: 0.4984 - val_accuracy: 0.8387\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4088 - accuracy: 0.8667 - val_loss: 0.5005 - val_accuracy: 0.8387\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4085 - accuracy: 0.8583 - val_loss: 0.5032 - val_accuracy: 0.8065\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4006 - accuracy: 0.8583 - val_loss: 0.5071 - val_accuracy: 0.8065\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3985 - accuracy: 0.8500 - val_loss: 0.5027 - val_accuracy: 0.8387\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3939 - accuracy: 0.8667 - val_loss: 0.4899 - val_accuracy: 0.8387\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3942 - accuracy: 0.8750 - val_loss: 0.4917 - val_accuracy: 0.8387\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3911 - accuracy: 0.8667 - val_loss: 0.5032 - val_accuracy: 0.8387\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3844 - accuracy: 0.8583 - val_loss: 0.4958 - val_accuracy: 0.8387\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3873 - accuracy: 0.8583 - val_loss: 0.4935 - val_accuracy: 0.8387\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3803 - accuracy: 0.8750 - val_loss: 0.5016 - val_accuracy: 0.8710\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3770 - accuracy: 0.8583 - val_loss: 0.5061 - val_accuracy: 0.8387\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3742 - accuracy: 0.8583 - val_loss: 0.4864 - val_accuracy: 0.8710\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3725 - accuracy: 0.8667 - val_loss: 0.4846 - val_accuracy: 0.8710\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3672 - accuracy: 0.9000 - val_loss: 0.5008 - val_accuracy: 0.9032\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3654 - accuracy: 0.8750 - val_loss: 0.5069 - val_accuracy: 0.8387\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3640 - accuracy: 0.8583 - val_loss: 0.4876 - val_accuracy: 0.9032\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3617 - accuracy: 0.8833 - val_loss: 0.4827 - val_accuracy: 0.8710\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3564 - accuracy: 0.8917 - val_loss: 0.4942 - val_accuracy: 0.8387\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3664 - accuracy: 0.8583 - val_loss: 0.5028 - val_accuracy: 0.8387\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3544 - accuracy: 0.8500 - val_loss: 0.4860 - val_accuracy: 0.8387\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3580 - accuracy: 0.8667 - val_loss: 0.4924 - val_accuracy: 0.8387\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3488 - accuracy: 0.8750 - val_loss: 0.5042 - val_accuracy: 0.9032\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3455 - accuracy: 0.8917 - val_loss: 0.4997 - val_accuracy: 0.8387\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3439 - accuracy: 0.8750 - val_loss: 0.4842 - val_accuracy: 0.8387\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3462 - accuracy: 0.8833 - val_loss: 0.4797 - val_accuracy: 0.9032\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3381 - accuracy: 0.9000 - val_loss: 0.4996 - val_accuracy: 0.8387\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3378 - accuracy: 0.8833 - val_loss: 0.5068 - val_accuracy: 0.8387\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3351 - accuracy: 0.8833 - val_loss: 0.4880 - val_accuracy: 0.8387\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3305 - accuracy: 0.9083 - val_loss: 0.4811 - val_accuracy: 0.8710\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3350 - accuracy: 0.8833 - val_loss: 0.4886 - val_accuracy: 0.8387\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3317 - accuracy: 0.8833 - val_loss: 0.4979 - val_accuracy: 0.8387\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3261 - accuracy: 0.8833 - val_loss: 0.4902 - val_accuracy: 0.8387\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3294 - accuracy: 0.8750 - val_loss: 0.4895 - val_accuracy: 0.8387\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3183 - accuracy: 0.9000 - val_loss: 0.4996 - val_accuracy: 0.8710\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3242 - accuracy: 0.8667 - val_loss: 0.5016 - val_accuracy: 0.8387\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2740 - accuracy: 0.9211\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train_normalized, y_train, epochs=200, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "# history = model.fit(X_train_normalized, y_train, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_normalized, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Test Accuracy: 0.9210526347160339\n",
      "Sample 1: Correct Predicted ALU with 99.68% likelihood\n",
      "Sample 2: Correct Predicted ALU with 99.93% likelihood\n",
      "Sample 3: Correct Predicted HDPE with 97.15% likelihood\n",
      "Sample 4: Correct Predicted HDPE with 96.58% likelihood\n",
      "Sample 5: Correct Predicted LDPE with 54.16% likelihood\n",
      "Sample 6: Correct Predicted PP with 60.26% likelihood\n",
      "Sample 7: Correct Predicted HDPE with 94.48% likelihood\n",
      "Sample 8: False Predicted PP with 56.77% likelihood\n",
      "Sample 9: Correct Predicted HDPE with 84.14% likelihood\n",
      "Sample 10: Correct Predicted PP with 80.16% likelihood\n",
      "Sample 11: Correct Predicted LDPE with 64.69% likelihood\n",
      "Sample 12: Correct Predicted PP with 62.61% likelihood\n",
      "Sample 13: Correct Predicted HDPE with 67.23% likelihood\n",
      "Sample 14: Correct Predicted ALU with 99.91% likelihood\n",
      "Sample 15: Correct Predicted LDPE with 55.46% likelihood\n",
      "Total size of the model: 74.52 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kylej\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test_normalized)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_plastic_types = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Convert predictions to percentage likelihood\n",
    "percentage_likelihood = predictions.max(axis=1) * 100\n",
    "\n",
    "# Display test accuracy and example predictions\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "for i in range(15):\n",
    "    if y_test[i] == predicted_labels[i]:\n",
    "        accuracy = \"Correct\"\n",
    "    else:\n",
    "        accuracy = \"False\"\n",
    "    print(f\"Sample {i+1}: {accuracy} Predicted {predicted_plastic_types[i]} with {percentage_likelihood[i]:.2f}% likelihood\")\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save(\"plastic_classifier_model.h5\")\n",
    "\n",
    "total_params = model.count_params()\n",
    "total_size_bytes = total_params * 4  # Each parameter is usually a 32-bit float\n",
    "total_size_mb = total_size_bytes / (1024)  # Convert bytes to megabytes\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "Test Accuracy: 0.9210526347160339\n",
      "Sample 1:\n",
      "  Prediction 1: ALU with 0.00% likelihood\n",
      "  Prediction 2: LDPE with 0.08% likelihood\n",
      "  Prediction 3: HDPE with 0.24% likelihood\n",
      "  Prediction 4: PP with 99.68% likelihood\n",
      "Sample 2:\n",
      "  Prediction 1: ALU with 0.00% likelihood\n",
      "  Prediction 2: LDPE with 0.00% likelihood\n",
      "  Prediction 3: HDPE with 0.07% likelihood\n",
      "  Prediction 4: PP with 99.93% likelihood\n",
      "Sample 3:\n",
      "  Prediction 1: HDPE with 0.01% likelihood\n",
      "  Prediction 2: PP with 1.40% likelihood\n",
      "  Prediction 3: LDPE with 1.44% likelihood\n",
      "  Prediction 4: ALU with 97.15% likelihood\n",
      "Sample 4:\n",
      "  Prediction 1: HDPE with 0.00% likelihood\n",
      "  Prediction 2: PP with 1.40% likelihood\n",
      "  Prediction 3: LDPE with 2.02% likelihood\n",
      "  Prediction 4: ALU with 96.58% likelihood\n",
      "Sample 5:\n",
      "  Prediction 1: LDPE with 0.12% likelihood\n",
      "  Prediction 2: PP with 10.04% likelihood\n",
      "  Prediction 3: HDPE with 35.68% likelihood\n",
      "  Prediction 4: ALU with 54.16% likelihood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kylej\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test_normalized)\n",
    "\n",
    "# Get the top 4 predictions for each sample\n",
    "top_predictions = np.argsort(predictions, axis=1)[:, -4:]\n",
    "top_labels = top_predictions[:, ::-1]\n",
    "\n",
    "# print(top_predictions)\n",
    "\n",
    "# Reshape the top_labels array to be 1D\n",
    "top_labels_1d = top_labels.reshape(-1)\n",
    "\n",
    "# Inverse transform the labels\n",
    "top_plastic_types = label_encoder.inverse_transform(top_labels_1d)\n",
    "\n",
    "# Reshape the top_plastic_types array back to 2D\n",
    "top_plastic_types = top_plastic_types.reshape(top_labels.shape)\n",
    "\n",
    "# Get the percentage likelihoods for the top 4 predictions\n",
    "percentage_likelihoods = np.take_along_axis(predictions, top_predictions, axis=1) * 100\n",
    "\n",
    "# Display test accuracy and example predictions\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "for i in range(5):\n",
    "    num_predictions = min(4, len(np.unique(top_labels[i])))\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    for j in range(num_predictions):\n",
    "        print(f\"  Prediction {j+1}: {top_plastic_types[i][j]} with {percentage_likelihoods[i][j]:.2f}% likelihood\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"plastic_classifier_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "[[0.5601391  0.00938571 0.365221   0.0652542 ]]\n",
      "0\n",
      "ALU\n",
      "Manual Input Prediction:\n",
      "  Prediction: ALU with 56.01% likelihood\n"
     ]
    }
   ],
   "source": [
    "# Get manual input from the user\n",
    "manual_input = np.array([[524161,1534725,2819437,3403538,4467703,2420628,2047009,1183458,860611,1677583,2394933,3207380,2819927,1889676,1374559]])  # Replace with your input values\n",
    "\n",
    "# Normalize the manual input if needed\n",
    "normalized_manual_input = ((manual_input - 0) / (8388608 - 0))\n",
    "\n",
    "# Make predictions on manual input\n",
    "predictions = model.predict(normalized_manual_input)\n",
    "print(predictions)\n",
    "\n",
    "# Get the top prediction\n",
    "top_prediction = np.argmax(predictions)\n",
    "print(top_prediction)\n",
    "\n",
    "top_plastic_type = label_encoder.inverse_transform([top_prediction])[0]\n",
    "print(top_plastic_type)\n",
    "likelihood = predictions[0][top_prediction] * 100\n",
    "\n",
    "# Display the prediction for manual input\n",
    "print(f\"Manual Input Prediction:\")\n",
    "print(f\"  Prediction: {top_plastic_type} with {likelihood:.2f}% likelihood\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
